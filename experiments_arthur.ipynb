{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from loading import load_data, load_test\n",
    "from preprocessing import preprocess_x, preprocess_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x_orig, data_y_orig = load_data() # loads from \".\\data\" by default\n",
    "test_x_orig = load_test() # this is for submission purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at missing data\n",
    "s = data_x_orig.isna().sum()/data_x_orig.count()\n",
    "s[data_x_orig.isna().any()].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = preprocess_x(data_x_orig)\n",
    "data_y = preprocess_y(data_x_orig, data_y_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give ids to traders\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "trader_encoder = LabelEncoder()\n",
    "groups = trader_encoder.fit_transform(data_x_orig[\"Trader\"].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send everything to numpy arrays\n",
    "X = data_x.to_numpy()\n",
    "y = data_y.to_numpy()"
   ]
  },
  {
   "source": [
    "## Model Selection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# build a list of dicts that says which classifier heads to test, and what params to test on them\n",
    "params = [\n",
    "    {\n",
    "        \"clf\": [RadiusNeighborsClassifier()], \n",
    "        \"clf__radius\": stats.loguniform(1, 1e2)\n",
    "    }, {\n",
    "        \"clf\": [ExtraTreesClassifier(max_features=\"sqrt\")],\n",
    "        \"clf__n_estimators\": stats.loguniform(50,200)\n",
    "    }, {\n",
    "        \"clf\": [SGDClassifier()],\n",
    "        \"clf__class_weight\": [None, \"balanced\"]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# these estimators will be appled sequentially to the data:\n",
    "pipe = Pipeline([\n",
    "    (\"standardisation\", StandardScaler()),\n",
    "    (\"reduce_dim\", PCA(n_components='mle')),\n",
    "    (\"clf\", SGDClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset into training and validation by trader\n",
    "gkf = GroupKFold(n_splits=5).split(X, y, groups)\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    n_iter=10,\n",
    "    cv=gkf,\n",
    "    estimator=pipe,\n",
    "    param_distributions=params,\n",
    "    scoring=\"balanced_accuracy\", # TODO is this the best score ? Shouldn't we implement our own method ?\n",
    "    n_jobs=-1,\n",
    "    pre_dispatch=\"2*n_jobs\"\n",
    ")\n",
    "\n",
    "search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}